{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_RF4dr8CXNAL",
        "outputId": "03e91ea2-e09a-4fba-eca5-a544025856ba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting vaderSentiment\n",
            "  Downloading vaderSentiment-3.3.2-py2.py3-none-any.whl (125 kB)\n",
            "\u001b[?25l\r\u001b[K     |██▋                             | 10 kB 27.8 MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 20 kB 32.7 MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 30 kB 32.4 MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 40 kB 22.1 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 51 kB 23.2 MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 61 kB 26.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 71 kB 20.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 81 kB 21.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 92 kB 23.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 102 kB 23.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 112 kB 23.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 122 kB 23.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 125 kB 23.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from vaderSentiment) (2.23.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->vaderSentiment) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->vaderSentiment) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->vaderSentiment) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->vaderSentiment) (2.10)\n",
            "Installing collected packages: vaderSentiment\n",
            "Successfully installed vaderSentiment-3.3.2\n"
          ]
        }
      ],
      "source": [
        "%pip3 install vaderSentiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "074Tafti3ce-",
        "outputId": "eda26f29-a8f0-47cd-dd4f-e9ffcd955697"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.7/dist-packages (3.6.0)\n",
            "Collecting gensim\n",
            "  Downloading gensim-4.2.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (24.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 24.1 MB 1.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (6.0.0)\n",
            "Requirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.21.6)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.4.1)\n",
            "Installing collected packages: gensim\n",
            "  Attempting uninstall: gensim\n",
            "    Found existing installation: gensim 3.6.0\n",
            "    Uninstalling gensim-3.6.0:\n",
            "      Successfully uninstalled gensim-3.6.0\n",
            "Successfully installed gensim-4.2.0\n",
            "Collecting tf-estimator-nightly==2.8.0.dev2021122109\n",
            "  Downloading tf_estimator_nightly-2.8.0.dev2021122109-py2.py3-none-any.whl (462 kB)\n",
            "\u001b[K     |████████████████████████████████| 462 kB 11.1 MB/s \n",
            "\u001b[?25hInstalling collected packages: tf-estimator-nightly\n",
            "Successfully installed tf-estimator-nightly-2.8.0.dev2021122109\n",
            "Requirement already satisfied: keras==2.8.0 in /usr/local/lib/python3.7/dist-packages (2.8.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (1.3.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (1.21.6)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2022.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n"
          ]
        }
      ],
      "source": [
        "%pip3 install gensim --upgrade\n",
        "%pip3 install tf-estimator-nightly==2.8.0.dev2021122109\n",
        "%pip3 install keras==2.8.0 --upgrade\n",
        "%pip3 install pandas --upgrade"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "hkC36Dg4XV52"
      },
      "outputs": [],
      "source": [
        "import socket\n",
        "import sys\n",
        "import requests\n",
        "import requests_oauthlib\n",
        "import json\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "import enum\n",
        "class Method(enum.Enum):\n",
        "   Vader = 1\n",
        "   LSTM = 2\n",
        "# Replace the values below with yours\n",
        "ACCESS_TOKEN = ''\n",
        "ACCESS_SECRET = ''\n",
        "CONSUMER_KEY = ''\n",
        "CONSUMER_SECRET = ''\n",
        "BEARER_TOKEN = 'AAAAAAAAAAAAAAAAAAAAAEt%2FcgEAAAAA%2FgThpZgB8X3Ds%2BJ20r4zjpeBpXI%3DIzrgb0nXRJf5h6Qbneg0z2rS7KLgU6NWQGgJiaqvkuf1zz4x2j'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "ZNDPIvbzgF85"
      },
      "outputs": [],
      "source": [
        "headers = {\"Authorization\": \"Bearer {}\".format(BEARER_TOKEN), \"content-type\": \"application/json\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "_RZQkjqvYDhb"
      },
      "outputs": [],
      "source": [
        "def get_tweets():\n",
        "    url = 'https://api.twitter.com/2/tweets/1228393702244134912'\n",
        "    query_data = [('tweet.fields', 'created_at,attachments'), ('expansions', 'author_id')]\n",
        "    query_url = url + '?' + '&'.join([str(t[0]) + '=' + str(t[1]) for t in query_data])\n",
        "    response = requests.get(query_url, headers=headers, stream=True)\n",
        "    print(query_url, response)\n",
        "    \n",
        "    return response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "53MZOuXOukkn"
      },
      "outputs": [],
      "source": [
        "def get_tweets_by_user(user):\n",
        "    url = 'https://api.twitter.com/2/tweets/search/recent'\n",
        "    query_data = [('query', 'from:'+user), ('max_results', '10')]\n",
        "    query_url = url + '?' + '&'.join([str(t[0]) + '=' + str(t[1]) for t in query_data])\n",
        "    response = requests.get(query_url, headers=headers, stream=True)\n",
        "    #print(query_url, response)\n",
        "    \n",
        "    return response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "Kp8TAYtOmV8t"
      },
      "outputs": [],
      "source": [
        "def get_tweet_comments(id='1526022409383460866'):\n",
        "    url = 'https://api.twitter.com/2/tweets/search/recent'\n",
        "    #url = 'https://api.twitter.com/2/tweets/search/all'\n",
        "    query_data = [('query', 'conversation_id:'+id), ('max_results', '10'), ('tweet.fields', 'conversation_id,in_reply_to_user_id,created_at')]\n",
        "    query_url = url + '?' + '&'.join([str(t[0]) + '=' + str(t[1]) for t in query_data])\n",
        "    response = requests.get(query_url, headers=headers, stream=True)\n",
        "    #print(query_url, response)\n",
        "    \n",
        "    return response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "wp7xi7JFaJZv"
      },
      "outputs": [],
      "source": [
        "analyzer = SentimentIntensityAnalyzer()\n",
        "\n",
        "def process_tweets_comments_vader(http_resp):\n",
        "    data = http_resp.json()\n",
        "    negative = 0\n",
        "    neutral = 0\n",
        "    positive = 0\n",
        "    if 'data' in data:\n",
        "      for comment in data['data']:\n",
        "          tweet_text = str(comment['text'].encode(\"utf-8\"))\n",
        "\n",
        "          # analysis sentiment score\n",
        "          sentiment_score = analyzer.polarity_scores(tweet_text)[\"compound\"]\n",
        "          if sentiment_score >= 0.05:\n",
        "              sentiment = \"POSITIVE\"\n",
        "              positive += 1\n",
        "          elif sentiment_score <= -0.05:\n",
        "              sentiment = \"NEGATIVE\"\n",
        "              negative += 1\n",
        "          else:\n",
        "              sentiment = \"NEUTRAL\"\n",
        "              neutral += 1\n",
        "\n",
        "          # separate sentiment label with tweet content\n",
        "          mess =  sentiment + '||||' + tweet_text + '\\n'\n",
        "          #print(mess)\n",
        "          #tcp_connection.send(bytes(mess, 'utf-8'))\n",
        "    return (positive,neutral,negative)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "ename": "ImportError",
          "evalue": "cannot import name 'pad_sequences' from 'keras.preprocessing.sequence' (c:\\Python310\\lib\\site-packages\\keras\\preprocessing\\sequence.py)",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[1;32md:\\repos\\sentiment-analysis\\model\\Sentimental_analysis_process_data.ipynb Cell 9'\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/repos/sentiment-analysis/model/Sentimental_analysis_process_data.ipynb#ch0000014?line=0'>1</a>\u001b[0m sys\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mappend(\u001b[39m\"\"\"\u001b[39m\u001b[39mD:/repos/sentiment-analysis/model\u001b[39m\u001b[39m\"\"\"\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/repos/sentiment-analysis/model/Sentimental_analysis_process_data.ipynb#ch0000014?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpreprocessing\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msequence\u001b[39;00m \u001b[39mimport\u001b[39;00m pad_sequences\n",
            "\u001b[1;31mImportError\u001b[0m: cannot import name 'pad_sequences' from 'keras.preprocessing.sequence' (c:\\Python310\\lib\\site-packages\\keras\\preprocessing\\sequence.py)"
          ]
        }
      ],
      "source": [
        "from keras.preprocessing.sequence import pad_sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OBX4IoBh1axN",
        "outputId": "da49657d-3181-4a63-dc21-9291b39d67e9"
      },
      "outputs": [
        {
          "ename": "ImportError",
          "evalue": "cannot import name 'pad_sequences' from 'keras.preprocessing.sequence' (c:\\Python310\\lib\\site-packages\\keras\\preprocessing\\sequence.py)",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[1;32md:\\repos\\sentiment-analysis\\model\\Sentimental_analysis_process_data.ipynb Cell 9'\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/repos/sentiment-analysis/model/Sentimental_analysis_process_data.ipynb#ch0000008?line=7'>8</a>\u001b[0m \u001b[39mfrom\u001b[39;00m  \u001b[39mnltk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mstem\u001b[39;00m \u001b[39mimport\u001b[39;00m SnowballStemmer\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/repos/sentiment-analysis/model/Sentimental_analysis_process_data.ipynb#ch0000008?line=8'>9</a>\u001b[0m \u001b[39m# Keras\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/repos/sentiment-analysis/model/Sentimental_analysis_process_data.ipynb#ch0000008?line=9'>10</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpreprocessing\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msequence\u001b[39;00m \u001b[39mimport\u001b[39;00m pad_sequences\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/repos/sentiment-analysis/model/Sentimental_analysis_process_data.ipynb#ch0000008?line=10'>11</a>\u001b[0m \u001b[39m# Word2vec\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/repos/sentiment-analysis/model/Sentimental_analysis_process_data.ipynb#ch0000008?line=11'>12</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mgensim\u001b[39;00m\n",
            "\u001b[1;31mImportError\u001b[0m: cannot import name 'pad_sequences' from 'keras.preprocessing.sequence' (c:\\Python310\\lib\\site-packages\\keras\\preprocessing\\sequence.py)"
          ]
        }
      ],
      "source": [
        "# DataFrame\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "# nltk\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from  nltk.stem import SnowballStemmer\n",
        "# Keras\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "# Word2vec\n",
        "import gensim\n",
        "# Utility\n",
        "import re\n",
        "import logging\n",
        "import time\n",
        "import pickle\n",
        "import string\n",
        "# Set log\n",
        "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
        "\n",
        "# TEXT CLEANING\n",
        "TEXT_CLEANING_RE = \"@\\S+|https?:\\S+|http?:\\S|[^A-Za-z0-9]+\"\n",
        "POSITIVE = \"POSITIVE\"\n",
        "NEGATIVE = \"NEGATIVE\"\n",
        "NEUTRAL = \"NEUTRAL\"\n",
        "SENTIMENT_THRESHOLDS = (0.4, 0.7)\n",
        "SEQUENCE_LENGTH = 300\n",
        "# EXPORT FILE\n",
        "KERAS_MODEL = \"model.h5\"\n",
        "WORD2VEC_MODEL = \"model.w2v\"\n",
        "TOKENIZER_MODEL = \"tokenizer.pkl\"\n",
        "\n",
        "nltk.download('stopwords')\n",
        "stop_words = stopwords.words(\"english\")\n",
        "stemmer = SnowballStemmer(\"english\")\n",
        "w2v_model = gensim.models.word2vec.Word2Vec.load(\"/content/drive/MyDrive/Thac_sy/HK3/HTTM/BTL/model.w2v\")\n",
        "tokenizer = pickle.load(open('/content/drive/MyDrive/Thac_sy/HK3/HTTM/BTL/tokenizer.pkl','rb'))\n",
        "model = keras.models.load_model(\"/content/drive/MyDrive/Thac_sy/HK3/HTTM/BTL/model.h5\")\n",
        "\n",
        "def remove_hashtag(sentence):\n",
        "  return re.sub(\"([@#][A-Za-z0-9_]+)\",\"\", sentence)\n",
        "\n",
        "def preprocess(text, stem=False):\n",
        "    # Remove link,user and special characters\n",
        "    text = re.sub(TEXT_CLEANING_RE, ' ', str(text).lower()).strip()\n",
        "    # Remove hashtag\n",
        "    text = re.sub(\"([@#][A-Za-z0-9_]+)\",\"\", text)\n",
        "    # Remove number\n",
        "    text = re.sub(\"(\\d+)\", '', text)\n",
        "    # Remove punctuation\n",
        "    punctuation = list(string.punctuation) + ['...',  '``']\n",
        "    tokens = []\n",
        "    for token in text.split():\n",
        "        if (token not in stop_words) and (token not in punctuation):\n",
        "            if stem:\n",
        "                tokens.append(stemmer.stem(token))\n",
        "            else:\n",
        "                tokens.append(token)\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "def decode_sentiment(score, include_neutral=True):\n",
        "    if include_neutral:        \n",
        "        label = NEUTRAL\n",
        "        if score <= SENTIMENT_THRESHOLDS[0]:\n",
        "            label = NEGATIVE\n",
        "        elif score >= SENTIMENT_THRESHOLDS[1]:\n",
        "            label = POSITIVE\n",
        "\n",
        "        return label\n",
        "    else:\n",
        "        return NEGATIVE if score < 0.5 else POSITIVE\n",
        "\n",
        "def predict(text, include_neutral=True):\n",
        "    start_at = time.time()\n",
        "    # Prepocess\n",
        "    text = preprocess(text)\n",
        "    # Tokenize text\n",
        "    x_test = pad_sequences(tokenizer.texts_to_sequences([text]), maxlen=SEQUENCE_LENGTH)\n",
        "    # Predict\n",
        "    score = model.predict([x_test])[0]\n",
        "    # Decode sentiment\n",
        "    label = decode_sentiment(score, include_neutral=include_neutral)\n",
        "\n",
        "    return {\"label\": label, \"score\": float(score),\n",
        "       \"elapsed_time\": time.time()-start_at}\n",
        "\n",
        "def process_tweets_comments_lstm(http_resp):\n",
        "    data = http_resp.json()\n",
        "    negative = 0\n",
        "    neutral = 0\n",
        "    positive = 0\n",
        "    if 'data' in data:\n",
        "      for comment in data['data']:\n",
        "          tweet_text = str(comment['text'].encode(\"utf-8\"))\n",
        "\n",
        "          # analysis sentiment score\n",
        "          label = predict(tweet_text)['label']\n",
        "          if label == \"POSITIVE\":\n",
        "              positive += 1\n",
        "          elif label == \"NEGATIVE\":\n",
        "              negative += 1\n",
        "          else:\n",
        "              neutral += 1\n",
        "          mess =  label + '||||' + tweet_text + '\\n'\n",
        "          #print(mess)\n",
        "    return (positive,neutral,negative)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "_mJ2UrI-vbuM"
      },
      "outputs": [],
      "source": [
        "def analyze_sentimental_recent_post_of_user(username, method=Method.Vader):\n",
        "    print('Analyzing for user %s ...' %(username))\n",
        "    resp_tw = get_tweets_by_user(username)\n",
        "    data = resp_tw.json()\n",
        "    print(data)\n",
        "    positive = 0\n",
        "    neutral = 0\n",
        "    negative = 0\n",
        "    if 'data' in data:\n",
        "        for tweet in data['data']:\n",
        "            #print(tweet['id'])\n",
        "            resp= get_tweet_comments(tweet['id'])\n",
        "            #print(resp)\n",
        "            if method == Method.Vader:\n",
        "              positive,neutral,negative = process_tweets_comments_vader(resp)\n",
        "            else:\n",
        "              positive,neutral,negative = process_tweets_comments_lstm(resp)\n",
        "            print('TWEET: %s \\nPOS: %d, NEU: %d, NEG: %d'%(tweet['text'],positive,neutral,negative))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M8asX08LtN_R",
        "outputId": "08e2d74a-adce-4ffa-b098-f5611b361cc1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Analyzing for user POTUS ...\n",
            "{'data': [{'id': '1526744379708293121', 'text': 'Today marks 68 years since Brown v. Board of Education — outlawing racial segregation in schools and bringing us closer to equal protection under the law. As we celebrate the landmark decision, we must continue our work to build a more just and equitable future for all Americans.'}, {'id': '1526736453140717570', 'text': 'Jill and I were honored to welcome folks to celebrate Asian American, Native Hawaiian, and Pacific Islander Heritage Month. It was a reminder that the reason America is so strong — the reason we are who we are — is because we’re diverse with so many talents. We can’t forget that. https://t.co/j7ZDKgZUFN'}, {'id': '1526699586659749888', 'text': 'Hate and fear are being given too much oxygen by those who pretend to love America, but who don’t understand America.\\n\\nTo confront the ideology of hate requires caring about all people. That’s the America I know: the most diverse, multi-racial, dynamic nation in history.'}, {'id': '1526687330198757379', 'text': 'Good news, folks: You can order another round of free COVID-19 tests shipped right to your door.\\n \\nVisit https://t.co/12LzJrqFsC to get yours.'}, {'id': '1526667251830726656', 'text': 'On this International Day Against Homophobia, Transphobia, and Biphobia, make no mistake: Hateful legislative attacks against the LGBTQI+ community cannot be tolerated in America or anywhere else.\\n\\nWe must continue to defend human rights and dignity, at home and around the world.'}, {'id': '1526657613261307905', 'text': 'Tune in as we host a reception to celebrate Asian American, Native Hawaiian, and Pacific Islander Heritage Month. https://t.co/47iLFreAiF'}, {'id': '1526641001091178498', 'text': 'We have to refuse to live in a country where Black people grocery shopping can be gunned down by weapons of war deployed in a racist cause.\\n\\nWe have to refuse to live in a country where fear and lies are packaged for power and for profit.'}, {'id': '1526627890539929602', 'text': 'White supremacy is a poison running through our body politic.\\n\\nWe need to say as clearly and forcefully as we can that the ideology of white supremacy has no place in America.\\n\\nFailure to do so will be complicity. Silence will be complicity. We cannot remain silent.'}, {'id': '1526617486774480897', 'text': 'In America, evil will not win.\\n\\nHate will not prevail.\\n\\nWhite supremacy will not have the last word.'}, {'id': '1526605600913752064', 'text': 'Jill and I are in Buffalo to stand with the community and to grieve with the families. As a nation, we must find purpose to live a life worthy of those we lost. We must resolve that from tragedy will come hope and light and life. https://t.co/Om8sTigHXl'}], 'meta': {'newest_id': '1526744379708293121', 'oldest_id': '1526605600913752064', 'result_count': 10, 'next_token': 'b26v89c19zqg8o3fpywnboqzta1u8tmuj9syxe9i32f7h'}}\n",
            "TWEET: Today marks 68 years since Brown v. Board of Education — outlawing racial segregation in schools and bringing us closer to equal protection under the law. As we celebrate the landmark decision, we must continue our work to build a more just and equitable future for all Americans. \n",
            "POS: 5, NEU: 2, NEG: 3\n",
            "TWEET: Jill and I were honored to welcome folks to celebrate Asian American, Native Hawaiian, and Pacific Islander Heritage Month. It was a reminder that the reason America is so strong — the reason we are who we are — is because we’re diverse with so many talents. We can’t forget that. https://t.co/j7ZDKgZUFN \n",
            "POS: 6, NEU: 1, NEG: 3\n",
            "TWEET: Hate and fear are being given too much oxygen by those who pretend to love America, but who don’t understand America.\n",
            "\n",
            "To confront the ideology of hate requires caring about all people. That’s the America I know: the most diverse, multi-racial, dynamic nation in history. \n",
            "POS: 3, NEU: 4, NEG: 3\n",
            "TWEET: Good news, folks: You can order another round of free COVID-19 tests shipped right to your door.\n",
            " \n",
            "Visit https://t.co/12LzJrqFsC to get yours. \n",
            "POS: 3, NEU: 2, NEG: 5\n",
            "TWEET: On this International Day Against Homophobia, Transphobia, and Biphobia, make no mistake: Hateful legislative attacks against the LGBTQI+ community cannot be tolerated in America or anywhere else.\n",
            "\n",
            "We must continue to defend human rights and dignity, at home and around the world. \n",
            "POS: 3, NEU: 1, NEG: 6\n",
            "TWEET: Tune in as we host a reception to celebrate Asian American, Native Hawaiian, and Pacific Islander Heritage Month. https://t.co/47iLFreAiF \n",
            "POS: 3, NEU: 4, NEG: 3\n",
            "TWEET: We have to refuse to live in a country where Black people grocery shopping can be gunned down by weapons of war deployed in a racist cause.\n",
            "\n",
            "We have to refuse to live in a country where fear and lies are packaged for power and for profit. \n",
            "POS: 2, NEU: 1, NEG: 7\n",
            "TWEET: White supremacy is a poison running through our body politic.\n",
            "\n",
            "We need to say as clearly and forcefully as we can that the ideology of white supremacy has no place in America.\n",
            "\n",
            "Failure to do so will be complicity. Silence will be complicity. We cannot remain silent. \n",
            "POS: 1, NEU: 6, NEG: 3\n",
            "TWEET: In America, evil will not win.\n",
            "\n",
            "Hate will not prevail.\n",
            "\n",
            "White supremacy will not have the last word. \n",
            "POS: 1, NEU: 3, NEG: 6\n",
            "TWEET: Jill and I are in Buffalo to stand with the community and to grieve with the families. As a nation, we must find purpose to live a life worthy of those we lost. We must resolve that from tragedy will come hope and light and life. https://t.co/Om8sTigHXl \n",
            "POS: 2, NEU: 4, NEG: 4\n"
          ]
        }
      ],
      "source": [
        "analyze_sentimental_recent_post_of_user('POTUS', Method.Vader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NwY26jbu3RZP",
        "outputId": "afe14ac3-dc86-4a8f-9f39-4b5188304318"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Analyzing for user POTUS ...\n",
            "{'data': [{'id': '1526744379708293121', 'text': 'Today marks 68 years since Brown v. Board of Education — outlawing racial segregation in schools and bringing us closer to equal protection under the law. As we celebrate the landmark decision, we must continue our work to build a more just and equitable future for all Americans.'}, {'id': '1526736453140717570', 'text': 'Jill and I were honored to welcome folks to celebrate Asian American, Native Hawaiian, and Pacific Islander Heritage Month. It was a reminder that the reason America is so strong — the reason we are who we are — is because we’re diverse with so many talents. We can’t forget that. https://t.co/j7ZDKgZUFN'}, {'id': '1526699586659749888', 'text': 'Hate and fear are being given too much oxygen by those who pretend to love America, but who don’t understand America.\\n\\nTo confront the ideology of hate requires caring about all people. That’s the America I know: the most diverse, multi-racial, dynamic nation in history.'}, {'id': '1526687330198757379', 'text': 'Good news, folks: You can order another round of free COVID-19 tests shipped right to your door.\\n \\nVisit https://t.co/12LzJrqFsC to get yours.'}, {'id': '1526667251830726656', 'text': 'On this International Day Against Homophobia, Transphobia, and Biphobia, make no mistake: Hateful legislative attacks against the LGBTQI+ community cannot be tolerated in America or anywhere else.\\n\\nWe must continue to defend human rights and dignity, at home and around the world.'}, {'id': '1526657613261307905', 'text': 'Tune in as we host a reception to celebrate Asian American, Native Hawaiian, and Pacific Islander Heritage Month. https://t.co/47iLFreAiF'}, {'id': '1526641001091178498', 'text': 'We have to refuse to live in a country where Black people grocery shopping can be gunned down by weapons of war deployed in a racist cause.\\n\\nWe have to refuse to live in a country where fear and lies are packaged for power and for profit.'}, {'id': '1526627890539929602', 'text': 'White supremacy is a poison running through our body politic.\\n\\nWe need to say as clearly and forcefully as we can that the ideology of white supremacy has no place in America.\\n\\nFailure to do so will be complicity. Silence will be complicity. We cannot remain silent.'}, {'id': '1526617486774480897', 'text': 'In America, evil will not win.\\n\\nHate will not prevail.\\n\\nWhite supremacy will not have the last word.'}, {'id': '1526605600913752064', 'text': 'Jill and I are in Buffalo to stand with the community and to grieve with the families. As a nation, we must find purpose to live a life worthy of those we lost. We must resolve that from tragedy will come hope and light and life. https://t.co/Om8sTigHXl'}], 'meta': {'newest_id': '1526744379708293121', 'oldest_id': '1526605600913752064', 'result_count': 10, 'next_token': 'b26v89c19zqg8o3fpywnboqzta1u8tmuj9syxe9i32f7h'}}\n",
            "TWEET: Today marks 68 years since Brown v. Board of Education — outlawing racial segregation in schools and bringing us closer to equal protection under the law. As we celebrate the landmark decision, we must continue our work to build a more just and equitable future for all Americans. \n",
            "POS: 2, NEU: 6, NEG: 2\n",
            "TWEET: Jill and I were honored to welcome folks to celebrate Asian American, Native Hawaiian, and Pacific Islander Heritage Month. It was a reminder that the reason America is so strong — the reason we are who we are — is because we’re diverse with so many talents. We can’t forget that. https://t.co/j7ZDKgZUFN \n",
            "POS: 5, NEU: 3, NEG: 2\n",
            "TWEET: Hate and fear are being given too much oxygen by those who pretend to love America, but who don’t understand America.\n",
            "\n",
            "To confront the ideology of hate requires caring about all people. That’s the America I know: the most diverse, multi-racial, dynamic nation in history. \n",
            "POS: 4, NEU: 2, NEG: 4\n",
            "TWEET: Good news, folks: You can order another round of free COVID-19 tests shipped right to your door.\n",
            " \n",
            "Visit https://t.co/12LzJrqFsC to get yours. \n",
            "POS: 3, NEU: 6, NEG: 1\n",
            "TWEET: On this International Day Against Homophobia, Transphobia, and Biphobia, make no mistake: Hateful legislative attacks against the LGBTQI+ community cannot be tolerated in America or anywhere else.\n",
            "\n",
            "We must continue to defend human rights and dignity, at home and around the world. \n",
            "POS: 3, NEU: 4, NEG: 3\n",
            "TWEET: Tune in as we host a reception to celebrate Asian American, Native Hawaiian, and Pacific Islander Heritage Month. https://t.co/47iLFreAiF \n",
            "POS: 2, NEU: 7, NEG: 1\n",
            "TWEET: We have to refuse to live in a country where Black people grocery shopping can be gunned down by weapons of war deployed in a racist cause.\n",
            "\n",
            "We have to refuse to live in a country where fear and lies are packaged for power and for profit. \n",
            "POS: 2, NEU: 7, NEG: 1\n",
            "TWEET: White supremacy is a poison running through our body politic.\n",
            "\n",
            "We need to say as clearly and forcefully as we can that the ideology of white supremacy has no place in America.\n",
            "\n",
            "Failure to do so will be complicity. Silence will be complicity. We cannot remain silent. \n",
            "POS: 1, NEU: 8, NEG: 1\n",
            "TWEET: In America, evil will not win.\n",
            "\n",
            "Hate will not prevail.\n",
            "\n",
            "White supremacy will not have the last word. \n",
            "POS: 3, NEU: 4, NEG: 3\n",
            "TWEET: Jill and I are in Buffalo to stand with the community and to grieve with the families. As a nation, we must find purpose to live a life worthy of those we lost. We must resolve that from tragedy will come hope and light and life. https://t.co/Om8sTigHXl \n",
            "POS: 0, NEU: 4, NEG: 6\n"
          ]
        }
      ],
      "source": [
        "analyze_sentimental_recent_post_of_user('POTUS', Method.LSTM)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fgxza4PC6q4O",
        "outputId": "eab68606-6a14-4c92-b10f-a287c8c2cfc2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'elapsed_time': 0.0995635986328125,\n",
              " 'label': 'NEUTRAL',\n",
              " 'score': 0.5466052889823914}"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "predict(\"@POTUS Your a racist POS SWAMP RAT\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Sentimental_analysis_process_data.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
    },
    "kernelspec": {
      "display_name": "Python 3.10.1 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
