{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "predict_model.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T97wS6vuUUW9"
      },
      "outputs": [],
      "source": [
        "!pip install gensim --upgrade\n",
        "!pip install tf-estimator-nightly==2.8.0.dev2021122109\n",
        "!pip install keras==2.8.0 --upgrade\n",
        "!pip install pandas --upgrade"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# DataFrame\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "# nltk\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from  nltk.stem import SnowballStemmer\n",
        "# Keras\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "# Word2vec\n",
        "import gensim\n",
        "# Utility\n",
        "import re\n",
        "import logging\n",
        "import time\n",
        "import pickle\n",
        "import string\n",
        "# Set log\n",
        "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
        "\n",
        "# TEXT CLEANING\n",
        "TEXT_CLEANING_RE = \"@\\S+|https?:\\S+|http?:\\S|[^A-Za-z0-9]+\"\n",
        "POSITIVE = \"POSITIVE\"\n",
        "NEGATIVE = \"NEGATIVE\"\n",
        "NEUTRAL = \"NEUTRAL\"\n",
        "SENTIMENT_THRESHOLDS = (0.4, 0.7)\n",
        "SEQUENCE_LENGTH = 300\n",
        "# EXPORT FILE\n",
        "KERAS_MODEL = \"model.h5\"\n",
        "WORD2VEC_MODEL = \"model.w2v\"\n",
        "TOKENIZER_MODEL = \"tokenizer.pkl\""
      ],
      "metadata": {
        "id": "WHJU9-X9bawW"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n",
        "stop_words = stopwords.words(\"english\")\n",
        "stemmer = SnowballStemmer(\"english\")\n",
        "w2v_model = gensim.models.word2vec.Word2Vec.load(\"/content/drive/MyDrive/Thac_sy/HK3/HTTM/BTL/model.w2v\")\n",
        "tokenizer = pickle.load(open('/content/drive/MyDrive/Thac_sy/HK3/HTTM/BTL/tokenizer.pkl','rb'))\n",
        "model = keras.models.load_model(\"/content/drive/MyDrive/Thac_sy/HK3/HTTM/BTL/model.h5\")\n",
        "\n",
        "def remove_hashtag(sentence):\n",
        "  return re.sub(\"([@#][A-Za-z0-9_]+)\",\"\", sentence)\n",
        "\n",
        "def preprocess(text, stem=False):\n",
        "    # Remove link,user and special characters\n",
        "    text = re.sub(TEXT_CLEANING_RE, ' ', str(text).lower()).strip()\n",
        "    # Remove hashtag\n",
        "    text = re.sub(\"([@#][A-Za-z0-9_]+)\",\"\", text)\n",
        "    # Remove number\n",
        "    text = re.sub(\"(\\d+)\", '', text)\n",
        "    # Remove punctuation\n",
        "    punctuation = list(string.punctuation) + ['...',  '``']\n",
        "    tokens = []\n",
        "    for token in text.split():\n",
        "        if (token not in stop_words) and (token not in punctuation):\n",
        "            if stem:\n",
        "                tokens.append(stemmer.stem(token))\n",
        "            else:\n",
        "                tokens.append(token)\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "def decode_sentiment(score, include_neutral=True):\n",
        "    if include_neutral:        \n",
        "        label = NEUTRAL\n",
        "        if score <= SENTIMENT_THRESHOLDS[0]:\n",
        "            label = NEGATIVE\n",
        "        elif score >= SENTIMENT_THRESHOLDS[1]:\n",
        "            label = POSITIVE\n",
        "\n",
        "        return label\n",
        "    else:\n",
        "        return NEGATIVE if score < 0.5 else POSITIVE\n",
        "\n",
        "def predict(text, include_neutral=True):\n",
        "    start_at = time.time()\n",
        "    # Prepocess\n",
        "    text = preprocess(text)\n",
        "    # Tokenize text\n",
        "    x_test = pad_sequences(tokenizer.texts_to_sequences([text]), maxlen=SEQUENCE_LENGTH)\n",
        "    # Predict\n",
        "    score = model.predict([x_test])[0]\n",
        "    # Decode sentiment\n",
        "    label = decode_sentiment(score, include_neutral=include_neutral)\n",
        "\n",
        "    return {\"label\": label, \"score\": float(score),\n",
        "       \"elapsed_time\": time.time()-start_at}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gEHYX9eAcORy",
        "outputId": "e7a6b85c-533a-4b56-c65a-8fb1d69dfcc3"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-18 14:54:18,554 : INFO : loading Word2Vec object from /content/drive/MyDrive/Thac_sy/HK3/HTTM/BTL/model.w2v\n",
            "2022-05-18 14:54:18,732 : INFO : loading wv recursively from /content/drive/MyDrive/Thac_sy/HK3/HTTM/BTL/model.w2v.wv.* with mmap=None\n",
            "2022-05-18 14:54:18,734 : INFO : setting ignored attribute cum_table to None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-18 14:54:19,153 : INFO : Word2Vec lifecycle event {'fname': '/content/drive/MyDrive/Thac_sy/HK3/HTTM/BTL/model.w2v', 'datetime': '2022-05-18T14:54:19.152939', 'gensim': '4.2.0', 'python': '3.7.13 (default, Apr 24 2022, 01:04:09) \\n[GCC 7.5.0]', 'platform': 'Linux-5.4.188+-x86_64-with-Ubuntu-18.04-bionic', 'event': 'loaded'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predict(\"I love the music! ...\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "61yZxyUeic6L",
        "outputId": "b1a054fe-1c1b-4e3c-8db0-50beb5eef27d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'elapsed_time': 0.8725259304046631,\n",
              " 'label': 'POSITIVE',\n",
              " 'score': 0.9656286239624023}"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    }
  ]
}